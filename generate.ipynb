{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf849a3b-ee86-4610-bb09-4ad4b64522b8",
   "metadata": {},
   "source": [
    "# This notebook is for *Live* models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733b27d-1845-4154-bca8-2ff3992e0046",
   "metadata": {},
   "source": [
    "### Model dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c3c25-4a6e-4454-be24-d9eb53388eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys,time,os\n",
    "\n",
    "import midi2audio\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from anticipation import ops\n",
    "from anticipation.sample import generate, control_prefix\n",
    "from anticipation.tokenize import extract_instruments\n",
    "from anticipation.convert import events_to_midi, midi_to_events_new, events_to_midi, compound_to_events, midi_to_compound_new\n",
    "# from anticipation.visuals import visualize # uses numpy < 2.0 which causes compatability errors with MLC\n",
    "from anticipation.config import *\n",
    "from anticipation.vocab import *\n",
    "from anticipation.vocabs.tripletmidi import vocab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from anticipation.sample import nucleus, debugchat_forward\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # Ignore on cluster. Needed for fluidsynth to work locally:\n",
    "    import os\n",
    "    # Add /opt/homebrew/bin/fluidsynth to PATH\n",
    "    os.environ['PATH'] += ':/opt/homebrew/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221ac03-17af-433d-a8e9-ffb5c7e38396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mlc_llm.testing.debug_chat import DebugChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152f3c9-5e44-4fd2-b679-fda04c03a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF models\n",
    "# AMT_MED = '/juice4/scr4/nlp/music/lakh-checkpoints/futile-think-tank-272/step-800000/hf'\n",
    "# INST_MODEL = '/juice4/scr4/nlp/music/prelim-checkpoints/triplet-live/step-98844/hf/' # from Feb\n",
    "INSTR_MED_BASELINE_HF = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-30/0ha1twnc/step-2000/hf'\n",
    "INSTR_MED_BASELINE_AR_HF = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-autoreg/7cxypt7a/step-2000/hf'\n",
    "LIVE = '/juice4/scr4/nlp/music/prelim-checkpoints/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/hf'\n",
    "\n",
    "# MLC models\n",
    "INSTR_MED_BASELINE_AR_MLC = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-autoreg/7cxypt7a/step-2000/mlc'\n",
    "INSTR_MED_BASELINE_AR_MLC_LIB = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-autoreg/7cxypt7a/step-2000/mlc/instr-finetune-autoreg-med.so'\n",
    "\n",
    "LIVE_MLC = '/juice4/scr4/nlp/music/prelim-checkpoints/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/mlc/'\n",
    "LIVE_MLC_LIB = '/juice4/scr4/nlp/music/prelim-checkpoints/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/mlc/mlc_cuda.so'\n",
    "\n",
    "# load an anticipatory music transformer\n",
    "model = AutoModelForCausalLM.from_pretrained(LIVE).cuda()\n",
    "\n",
    "# load an anticipatory music transformer with MLC\n",
    "class DummyDebugInstrument:\n",
    "    def __init__(self, debug_out: Path):\n",
    "        self.debug_out = debug_out\n",
    "        pass\n",
    "\n",
    "    def reset(self, debug_out: Path):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, func, name, before_run, ret_val, *args):\n",
    "        pass\n",
    "        \n",
    "model_mlc = DebugChat(\n",
    "    model=LIVE_MLC,\n",
    "    debug_dir=Path(\"./debug-anticipation\"),\n",
    "    model_lib=LIVE_MLC_LIB,\n",
    "    debug_instrument=DummyDebugInstrument(Path(\"./debug-anticipation\"))\n",
    ")\n",
    "\n",
    "# a MIDI synthesizer\n",
    "fs = midi2audio.FluidSynth('/usr/share/sounds/sf2/FluidR3_GM.sf2')\n",
    "\n",
    "# the MIDI synthesis script\n",
    "def synthesize(fs, tokens):\n",
    "    mid = events_to_midi(tokens, vocab)\n",
    "    mid.save('tmp.mid')\n",
    "    fs.midi_to_audio('tmp.mid', 'tmp.wav')\n",
    "    return 'tmp.wav'\n",
    "\n",
    "def synthesize_miditoolkit(fs, mf):\n",
    "    mf.dump('tmp.mid')\n",
    "    fs.midi_to_audio('tmp.mid', 'tmp.wav')\n",
    "    return 'tmp.wav'\n",
    "\n",
    "# Remove prefix by finding the first index that is either within the TIME block or ATIME block\n",
    "def remove_prefix(tokens):\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if (tok in list(range(vocab['time_offset'], vocab['time_offset'] + vocab['config']['max_time']))) or (tok in list(range(vocab['atime_offset'], vocab['atime_offset'] + vocab['config']['max_time']))):\n",
    "            return tokens[i:]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151860ea-2406-42f2-b6a3-a1c68f7ea1b4",
   "metadata": {},
   "source": [
    "### Chorder dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de240fff-e4fd-41b7-911e-c27263c8b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chorder.chorder import Chord, Dechorder, chord_to_midi, play_chords\n",
    "from miditoolkit import MidiFile\n",
    "from copy import deepcopy\n",
    "chord_program_num = vocab['chord_instrument'] - vocab['instrument_offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b6159-a36d-4775-9850-524865dd86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_human_and_chords(midifile_path, human_program_num=None, return_non_human_events=False):\n",
    "    chord_program_num = vocab['chord_instrument'] - vocab['instrument_offset']\n",
    "\n",
    "    if human_program_num is not None:\n",
    "        # Extract human part\n",
    "        events = midi_to_events_new(midifile_path, vocab)\n",
    "        non_human_events, human_events = extract_instruments(events, [human_program_num])\n",
    "    else:\n",
    "        human_events = None\n",
    "\n",
    "    # Harmonize and assign chords to chord_program_num\n",
    "    mf = MidiFile(midifile_path)\n",
    "    mf_copy = deepcopy(mf) # chorder operations are done in-place\n",
    "    for instr in mf_copy.instruments:\n",
    "        if instr.program == human_program_num:\n",
    "            mf_copy.instruments.remove(instr)\n",
    "    mf_enchord = Dechorder.enchord(mf_copy)\n",
    "    mf_chords = play_chords(mf_enchord) \n",
    "    mf_chords.instruments[0].program = chord_program_num\n",
    "    mf.instruments = mf_chords.instruments # put back in original mf to preserve metadata\n",
    "    mf.dump('tmp.mid')\n",
    "    chord_events = compound_to_events(midi_to_compound_new('tmp.mid', vocab, debug=False)[0], vocab)\n",
    "    _, chord_events = extract_instruments(chord_events, [chord_program_num])\n",
    "\n",
    "    if return_non_human_events:\n",
    "        return (human_events, chord_events, non_human_events)\n",
    "\n",
    "    return (human_events, chord_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3478919-727b-4030-bb48-9f9e45a47f5d",
   "metadata": {},
   "source": [
    "### Basic autoregressive generation from a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ead631-d763-4c3b-a696-7024bb28659d",
   "metadata": {},
   "source": [
    "#### Optionally, choose an example from the train or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7208e3b-f75e-44db-9c0b-a2eb6cc3662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmd_tokens_path = '/juice4/scr4/nlp/music/npbecker/lmd_full_tokens/06232024/06232024_valid.autoregress.valid.txt' # instr baseline, 30x augmentation\n",
    "# lmd_tokens_path = '/juice4/scr4/nlp/music/npbecker/lmd_full_tokens/07092024/06232024.autoregress.train.txt' # instr autoregressive baseline, 1x augmentation\n",
    "# lmd_tokens_path = '/nlp/scr/npbecker/lmd_valid/tokenized-events-0.txt'\n",
    "lmd_tokens_path = '/nlp/scr/npbecker/lmd_full_tokens/06042024/06042024.autoregress.train.txt' # instr baseline, 30x augmentation\n",
    "\n",
    "chunks = []\n",
    "with open(lmd_tokens_path, 'r') as file:\n",
    "    for i in range(100000):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        chunks.append(line);\n",
    "\n",
    "tokenss = []\n",
    "start_tokens = []\n",
    "for chunk in chunks:\n",
    "    tokens = [int(tok) for tok in chunk.strip('\\n').split(' ')]\n",
    "    tokenss.append(tokens)\n",
    "    if tokens[1] == vocab['separator']:\n",
    "        start_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62d95d-299b-4d6f-b028-c99184296035",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = start_tokens[11]\n",
    "ops.print_training_tokens(t[:t.index(55026)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa6294-82ce-4d19-a1ce-8070b85a77d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = start_tokens[11]\n",
    "# visualize(ops.remove_prefix(t), 'tmp.png', vocab)\n",
    "# ops.print_training_tokens(t[:t.index(55026)+1])\n",
    "ops.print_training_tokens(t)\n",
    "e, c = ops.split(ops.remove_prefix(t))\n",
    "chords, piano = extract_instruments([t - vocab['control_offset'] for t in c], [0], as_controls=False)\n",
    "# Audio(synthesize(fs, ops.remove_prefix(t)))\n",
    "Audio(synthesize(fs, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b11b60-7247-48c9-a446-21063caeb4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove drums\n",
    "tokens = t[t.index(55026)+1:]\n",
    "prelim_e, c = ops.split(tokens)\n",
    "# get rid of drums\n",
    "e = []\n",
    "for time, dur, note in zip(prelim_e[0::3],prelim_e[1::3],prelim_e[2::3]):\n",
    "    instr = (note-NOTE_OFFSET)//2**7\n",
    "    if instr not in [128]:\n",
    "        e.extend([time, dur, note])\n",
    "\n",
    "# hack to deal with REST token at zero from time relativizing after padding\n",
    "zero_rest = e[:3]\n",
    "e = e[3:]\n",
    "# generate new control prefix without drums\n",
    "z_start, z_cont = control_prefix([32, 56], [0], 'autoregress', vocab)\n",
    "prefix = [vocab['pad']] + z_start\n",
    "# merge everything back together\n",
    "chords, human = extract_instruments([tok-CONTROL_OFFSET for tok in c], [0], as_controls=False)\n",
    "new_t, _, _ = ops.anticipate_and_anti_anticipate(e, [tok + CONTROL_OFFSET for tok in chords], [tok + CONTROL_OFFSET for tok in human])\n",
    "new_t = prefix + zero_rest + new_t\n",
    "ops.print_training_tokens(new_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66c6b1-ff99-43c3-bd7e-345292a09d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, piano))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac7385-7c19-46ed-8433-d865723053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbf5c2-faa1-4c88-9cd5-776309e470ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, new_t[new_t.index(55026)+1:301]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31141038-e499-485c-ae38-67c15ee40248",
   "metadata": {},
   "source": [
    "#### Basic autoregressive generation. To start from scratch, prompt with vocab['pad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d2827-975c-42ef-bbc4-e9d44f194714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_MLC = True\n",
    "\n",
    "completed_seq_length = 1 + 1*999 # should =1(mod3), and less than 1024\n",
    "top_p = .99\n",
    "temperature = 1.0\n",
    "\n",
    "# chunk = chunks[3000]\n",
    "# tokens = [int(tok) for tok in chunk.strip('\\n').split(' ')]\n",
    "# t[:t.index(55026) + 1] # get prompt from example t\n",
    "tokens = [vocab['pad']]\n",
    "\n",
    "# ==================================\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "while(len(tokens) < completed_seq_length):    \n",
    "    \n",
    "    new_token = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            if not use_MLC:\n",
    "                input_tokens = torch.tensor(tokens + new_token).unsqueeze(0).to(model.device)\n",
    "                logits = model(input_tokens).logits[0,-1]\n",
    "            else:\n",
    "                # MLC with no caching\n",
    "                input_tokens = torch.tensor(tokens + new_token)\n",
    "                logits, _ = debugchat_forward(model_mlc, input_tokens, None)\n",
    "                logits = torch.tensor(logits)[0,0,:]\n",
    "\n",
    "            logits = nucleus(logits, top_p)\n",
    "\n",
    "            probs = F.softmax(logits/temperature, dim=-1)\n",
    "            token = torch.multinomial(probs, 1)\n",
    "            new_token.append(int(token))\n",
    "            \n",
    "    tokens.extend(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63c18f-8272-485b-b10d-7a153c5bcb6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ops.print_training_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ba1ad-ffa2-4840-95f8-545bcf3be7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "to = tokens[tokens.index(55026)+1:]\n",
    "e, c = ops.split(to)\n",
    "Audio(synthesize(fs, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb0c1a-2ee3-47eb-b476-5904b5f9e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, piano = extract_instruments([tok-CONTROL_OFFSET for tok in c], [0], as_controls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ff85a-8a43-4f7c-adbf-e592c2d69c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, piano))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1a79a-c26d-4780-a53f-1aa723ab7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, e + piano))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8ce77-595f-4f33-8332-96410d975c00",
   "metadata": {},
   "source": [
    "### Unconditional generation with requested instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1a189-5666-459b-8a03-b866a8f5cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events = midi_to_events('examples/chopin_test_clipped.mid', vocab)\n",
    "# mt = ops.max_time(events, seconds=True)\n",
    "# length = 50\n",
    "# tokens = generate(model_mlc, inputs=events, start_time=mt, end_time=mt+length, human_instruments=[], instruments=[0], top_p=.98, use_MLC=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f118f0c-38b5-4080-8727-4b8bee945453",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_MLC=True\n",
    "length = 50\n",
    "acc_instruments = [2, 24, 46] #list(ops.get_instruments(events).keys())[:10]\n",
    "human_instruments = [] # this is empty for instrument med baseline\n",
    "if not use_MLC:\n",
    "    unconditional_tokens = generate(model, start_time=0, end_time=length, human_instruments=human_instruments, instruments=acc_instruments, top_p=.98, use_MLC=False)\n",
    "else:\n",
    "    unconditional_tokens = generate(model_mlc, start_time=0, end_time=length, human_instruments=human_instruments, instruments=acc_instruments, top_p=.98, use_MLC=True)\n",
    "sampled_instruments = ops.get_instruments(unconditional_tokens)\n",
    "print(f'Generated {len(unconditional_tokens)} tokens.')\n",
    "print(f'Requested instruments: {sorted(acc_instruments)}')\n",
    "print(f'Sampled instruments:')\n",
    "for key in sorted(sampled_instruments.keys()):\n",
    "    print(f'    Program {key} with {sampled_instruments[key]} notes')\n",
    "print('Accuracy:')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn in acc_instruments])} instruments out of {len(acc_instruments)} requested instruments generated')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn not in acc_instruments])} instruments generated that were not requested')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11731368-f105-47ed-b00f-af16b69e5713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, unconditional_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c297c5a6-913e-4d21-b47d-ba91e53bf143",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Running the model from a single separator token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f35c53-dc96-4403-adbc-dbfb6a3ac69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Instrument offset: ', vocab['instrument_offset'])\n",
    "print('Separator token: ', vocab['separator'])\n",
    "print('Pad token: ', vocab['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf69d0-1069-437e-864c-d1045c55359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from anticipation.sample import safe_logits, future_logits, instr_logits, nucleus\n",
    "\n",
    "length = 20\n",
    "top_p=1.0\n",
    "temperature=1.0\n",
    "debug=True\n",
    "\n",
    "tokens = [vocab['separator']]\n",
    "\n",
    "for _ in range(length):\n",
    "    new_token = []\n",
    "    current_time = ops.max_time(tokens, seconds=False)\n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            input_tokens = torch.tensor(tokens + new_token).unsqueeze(0).to(model.device)\n",
    "            logits = model(input_tokens).logits[0,-1]\n",
    "\n",
    "            idx = input_tokens.shape[1]-1\n",
    "            # logits = safe_logits(logits, idx)\n",
    "            # if i == 0:\n",
    "            #     logits = future_logits(logits, current_time)\n",
    "            # elif i == 2:\n",
    "            #     logits = instr_logits(logits, tokens)\n",
    "            # logits = masked_instr_logits(logits, masked_instrs)\n",
    "            logits = nucleus(logits, top_p)\n",
    "\n",
    "            probs = F.softmax(logits/temperature, dim=-1)\n",
    "            token = torch.multinomial(probs, 1)\n",
    "            new_token.append(int(token))\n",
    "\n",
    "    tokens.extend(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b195124-549f-4558-8135-39239a284123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unconditional_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b5511-cfc3-4849-a98d-a48e268fabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a14750-3efe-4e4f-91b1-e827beb77e17",
   "metadata": {},
   "source": [
    "### First LIVE example: Strawberry Fields (unfinished transfer over to Live model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789ac9d-5efa-4b30-9f49-e64d6b635b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "events = midi_to_events_new('examples/strawberry.mid', vocab)\n",
    "Audio(synthesize(fs, ops.clip(events, 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e23f7-a22c-4d0e-a9e3-ea2f641f18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a chord accompaniment (\"lead sheet\") and extrat the piano\n",
    "# part as simulated human input.\n",
    "# The chord accompaniment will be given to the model as anticipated controls.\n",
    "# The human accompaniment will be given to the model as anti-anticipated controls.\n",
    "human_instruments = [0]\n",
    "human_events, chord_events = extract_human_and_chords('examples/strawberry.mid', human_program_num=0, return_non_human_events=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513a07b-de8b-4a56-8649-78a42fd08145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticipation.visuals import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a217bd0-9f29-46ef-9ded-0a48f675202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the human part (piano)\n",
    "visualize(ops.clip([tk - CONTROL_OFFSET for tk in human_events], 0, 30), '', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a918e0-6597-436f-a2f4-adcd33c658ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, ops.clip([tk - CONTROL_OFFSET for tk in human_events], 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca55790-0393-415e-bcdf-8ddcd0e39b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the genereated chord accompaniment\n",
    "visualize(ops.clip([tk - CONTROL_OFFSET for tk in chord_events], 0, 30), '', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64762f-757d-4e80-aed9-0d407206358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render chords as instrument=101 (special program num just for lead sheets)\n",
    "Audio(synthesize(fs, ops.clip([tk - CONTROL_OFFSET for tk in chord_events], 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088b4ae-21cf-4ac9-853d-ed2917875def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human part (piano) + Chords \n",
    "Audio(synthesize(fs, ops.clip(ops.sort([tk - CONTROL_OFFSET for tk in chord_events] + [tk - CONTROL_OFFSET for tk in human_events]), 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179337c9-758e-4380-91b2-bb12b9dd245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are some reasonable instruments to try? Let's look at the original.\n",
    "list(ops.get_instruments(events).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f087703-beb0-4009-b940-1515763465b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 30\n",
    "requested_instruments = [25, 48]\n",
    "tokens = generate(model, chord_controls=chord_events, human_controls=human_events, start_time=0, end_time=length, human_instruments=human_instruments, instruments=requested_instruments, top_p=.98)\n",
    "print('Tokens generated: ',len(tokens))\n",
    "sampled_instruments = ops.get_instruments(tokens)\n",
    "print(f'Generated {len(tokens)} tokens.')\n",
    "print(f'Requested instruments: {sorted(requested_instruments)}')\n",
    "print(f'Sampled instruments:')\n",
    "for key in sorted(sampled_instruments.keys()):\n",
    "    print(f'    Program {key} with {sampled_instruments[key]} notes')\n",
    "print('Accuracy:')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn in requested_instruments])} instruments out of {len(requested_instruments)} requested instruments generated')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn not in requested_instruments])} instruments generated that were not requested')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b17467-6af1-4616-98fa-3660394d56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accompaniment\n",
    "Audio(synthesize(fs, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48674436-3acc-4fb2-a537-ebfbf739e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human part + Accompaniment\n",
    "Audio(synthesize(fs, ops.clip(ops.sort(tokens + [tk - CONTROL_OFFSET for tk in human_events]), 0, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4876b7-abce-47e8-bbeb-4cacb99cffbe",
   "metadata": {},
   "source": [
    "### Second Live example: jazz from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3573a-ceac-4ab8-8f70-356c136847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"b0ea637882ee7911da70d75f0b726992.mid\"\n",
    "human_instr = 0\n",
    "original = os.path.join(\"/nlp/scr/npbecker/lmd_train/b/\", filename)\n",
    "original_events = midi_to_events_new(original)\n",
    "# let's take out the drums\n",
    "original_events, _ = extract_instruments(original_events, [128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb803a3-7370-45ac-8dfe-e054a29adbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, original_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99303412-986a-42e1-9d9f-0b39c75096c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_events, chord_events, agent_events = extract_human_and_chords(original, human_program_num=human_instr, return_non_human_events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5a65c-a1ab-45a8-9352-ed4d91f7f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNTESTED\n",
    "\n",
    "start_time = 20\n",
    "end_time = 60\n",
    "\n",
    "human_controls = ops.clip(human_events,     0, end_time,                 clip_duration=False, seconds=True)\n",
    "inputs         = ops.clip(agent_events,     0, start_time,               clip_duration=False, seconds=True)\n",
    "chord_controls = ops.clip(chord_events,     0, end_time,                 clip_duration=False, seconds=True)\n",
    "\n",
    "requested_instruments = sorted(list(ops.get_instruments(agent_events).keys()))\n",
    "human_instruments = [human_instr]\n",
    "\n",
    "accompaniment = generate(\n",
    "    model, \n",
    "    inputs=inputs, \n",
    "    chord_controls=chord_controls, \n",
    "    human_controls=human_controls, \n",
    "    start_time=start_time, \n",
    "    end_time=end_time, \n",
    "    instruments=requested_instruments, \n",
    "    human_instruments=human_instruments, \n",
    "    top_p=.99, \n",
    "    masked_instrs=list(set(range(129)) - set(requested_instruments)),\n",
    "    allowed_control_pn=None,\n",
    "    debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f998420-6d79-49bb-bca4-e4da2b26d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, accompaniment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f0d91-5887-4c35-8cd7-ada95166b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, ops.sort(accompaniment + [tok - vocab['control_offset'] for tok in human_controls])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
