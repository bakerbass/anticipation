{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf849a3b-ee86-4610-bb09-4ad4b64522b8",
   "metadata": {},
   "source": [
    "# This notebook is for *Live* models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733b27d-1845-4154-bca8-2ff3992e0046",
   "metadata": {},
   "source": [
    "### Model dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c3c25-4a6e-4454-be24-d9eb53388eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys,time,os\n",
    "\n",
    "import midi2audio\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from anticipation import ops\n",
    "from anticipation.sample import generate, control_prefix\n",
    "from anticipation.tokenize import extract_instruments\n",
    "from anticipation.convert import events_to_midi, midi_to_events_new, events_to_midi, compound_to_events, midi_to_compound_new\n",
    "# from anticipation.visuals import visualize # uses numpy < 2.0 which causes compatability errors with MLC\n",
    "from anticipation.config import *\n",
    "from anticipation.vocab import *\n",
    "from anticipation.vocabs.tripletmidi import vocab\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from anticipation.sample import nucleus, debugchat_forward\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # Ignore on cluster. Needed for fluidsynth to work locally:\n",
    "    import os\n",
    "    # Add /opt/homebrew/bin/fluidsynth to PATH\n",
    "    os.environ['PATH'] += ':/opt/homebrew/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221ac03-17af-433d-a8e9-ffb5c7e38396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mlc_llm.testing.debug_chat import DebugChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152f3c9-5e44-4fd2-b679-fda04c03a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF models\n",
    "# AMT_MED = '/juice4/scr4/nlp/music/lakh-checkpoints/futile-think-tank-272/step-800000/hf'\n",
    "# INST_MODEL = '/juice4/scr4/nlp/music/prelim-checkpoints/triplet-live/step-98844/hf/' # from Feb\n",
    "INSTR_MED_BASELINE_HF = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-30/0ha1twnc/step-2000/hf'\n",
    "INSTR_MED_BASELINE_AR_HF = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-autoreg/7cxypt7a/step-2000/hf'\n",
    "# LIVE = '/juice4/scr4/nlp/music/prelim-checkpoints/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/hf'\n",
    "\n",
    "# MLC models\n",
    "INSTR_MED_BASELINE_AR_MLC = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-autoreg/7cxypt7a/step-2000/mlc'\n",
    "INSTR_MED_BASELINE_AR_MLC_LIB = '/juice4/scr4/nlp/music/prelim-checkpoints/instr-finetune-autoreg/7cxypt7a/step-2000/mlc/instr-finetune-autoreg-med.so'\n",
    "\n",
    "# LIVE_MLC = '/juice4/scr4/nlp/music/prelim-checkpoints/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/mlc/'\n",
    "# LIVE_MLC_LIB = '/juice4/scr4/nlp/music/prelim-checkpoints/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/mlc/mlc_cuda.so'\n",
    "\n",
    "# Local:\n",
    "LIVE = '/Users/npb/Desktop/anticipation/anticipation/mlc_music_models/models/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/hf'\n",
    "LIVE_MLC = '/Users/npb/Desktop/anticipation/anticipation/mlc_music_models/models/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/mlc'\n",
    "LIVE_MLC_LIB = '/Users/npb/Desktop/anticipation/anticipation/mlc_music_models/models/live-finetune-piano-aug-0604-med/1eaqb2uc/step-2000/mlc/q0f16-metal.so'\n",
    "\n",
    "# load an anticipatory music transformer\n",
    "if not torch.cuda.is_available():\n",
    "    model = AutoModelForCausalLM.from_pretrained(LIVE)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(LIVE).cuda()\n",
    "\n",
    "# load an anticipatory music transformer with MLC\n",
    "class DummyDebugInstrument:\n",
    "    def __init__(self, debug_out: Path):\n",
    "        self.debug_out = debug_out\n",
    "        pass\n",
    "\n",
    "    def reset(self, debug_out: Path):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, func, name, before_run, ret_val, *args):\n",
    "        pass\n",
    "        \n",
    "model_mlc = DebugChat(\n",
    "    model=LIVE_MLC,\n",
    "    debug_dir=Path(\"./debug-anticipation\"),\n",
    "    model_lib=LIVE_MLC_LIB,\n",
    "    debug_instrument=DummyDebugInstrument(Path(\"./debug-anticipation\"))\n",
    ")\n",
    "\n",
    "# a MIDI synthesizer\n",
    "fs = midi2audio.FluidSynth('/usr/share/sounds/sf2/FluidR3_GM.sf2')\n",
    "\n",
    "# the MIDI synthesis script\n",
    "def synthesize(fs, tokens):\n",
    "    mid = events_to_midi(tokens, vocab)\n",
    "    mid.save('tmp.mid')\n",
    "    fs.midi_to_audio('tmp.mid', 'tmp.wav')\n",
    "    return 'tmp.wav'\n",
    "\n",
    "def synthesize_miditoolkit(fs, mf):\n",
    "    mf.dump('tmp.mid')\n",
    "    fs.midi_to_audio('tmp.mid', 'tmp.wav')\n",
    "    return 'tmp.wav'\n",
    "\n",
    "# Remove prefix by finding the first index that is either within the TIME block or ATIME block\n",
    "def remove_prefix(tokens):\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if (tok in list(range(vocab['time_offset'], vocab['time_offset'] + vocab['config']['max_time']))) or (tok in list(range(vocab['atime_offset'], vocab['atime_offset'] + vocab['config']['max_time']))):\n",
    "            return tokens[i:]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151860ea-2406-42f2-b6a3-a1c68f7ea1b4",
   "metadata": {},
   "source": [
    "### Chorder dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de240fff-e4fd-41b7-911e-c27263c8b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chorder.chorder import Chord, Dechorder, chord_to_midi, play_chords\n",
    "from miditoolkit import MidiFile\n",
    "from copy import deepcopy\n",
    "chord_program_num = vocab['chord_instrument'] - vocab['instrument_offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b6159-a36d-4775-9850-524865dd86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_human_and_chords(midifile_path, human_program_num=None, return_non_human_events=False, remove_drums=True, relativize_time=True):\n",
    "    chord_program_num = vocab['chord_instrument'] - vocab['instrument_offset']\n",
    "\n",
    "    if return_non_human_events:\n",
    "        assert human_program_num is not None, \"Must provide human_program_num if return_non_human_events is True\"\n",
    "\n",
    "    if relativize_time:\n",
    "        assert human_program_num is not None, \"Must provide human_program_num if relativize_time is True; this is because the time offset is calculated based on the human part rn.\"\n",
    "\n",
    "    if human_program_num is not None:\n",
    "        # Extract human part\n",
    "        events = midi_to_events_new(midifile_path, vocab)\n",
    "        if remove_drums:\n",
    "            events, _ = extract_instruments(events, [128])\n",
    "        if relativize_time:\n",
    "            min_time = -ops.min_time(events, seconds=False)\n",
    "            events = ops.translate(events, min_time, seconds=False)\n",
    "        non_human_events, human_events = extract_instruments(events, [human_program_num])\n",
    "    else:\n",
    "        human_events = None\n",
    "\n",
    "    # Harmonize and assign chords to chord_program_num\n",
    "    mf = MidiFile(midifile_path)\n",
    "    if remove_drums:\n",
    "        mf.instruments = [instr for instr in mf.instruments if instr.is_drum != True]\n",
    "    mf_copy = deepcopy(mf) # chorder operations are done in-place\n",
    "    for instr in mf_copy.instruments:\n",
    "        if instr.program == human_program_num:\n",
    "            mf_copy.instruments.remove(instr)\n",
    "    mf_enchord = Dechorder.enchord(mf_copy)\n",
    "    mf_chords = play_chords(mf_enchord) \n",
    "    mf_chords.instruments[0].program = chord_program_num\n",
    "    mf.instruments = mf_chords.instruments # put back in original mf to preserve metadata\n",
    "    mf.dump('tmp.mid')\n",
    "    chord_events = compound_to_events(midi_to_compound_new('tmp.mid', vocab, debug=False)[0], vocab)\n",
    "    _, chord_events = extract_instruments(chord_events, [chord_program_num])\n",
    "\n",
    "    if relativize_time:\n",
    "        chord_events = ops.translate(chord_events, min_time, seconds=False)\n",
    "\n",
    "    if return_non_human_events:\n",
    "        return (human_events, chord_events, non_human_events)\n",
    "\n",
    "    return (human_events, chord_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3478919-727b-4030-bb48-9f9e45a47f5d",
   "metadata": {},
   "source": [
    "### Basic autoregressive generation from a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ead631-d763-4c3b-a696-7024bb28659d",
   "metadata": {},
   "source": [
    "#### Optionally, choose an example from the train or test set. (paths are currently set for cluster usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7208e3b-f75e-44db-9c0b-a2eb6cc3662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmd_tokens_path = '/juice4/scr4/nlp/music/npbecker/lmd_full_tokens/06232024/06232024_valid.autoregress.valid.txt' # instr baseline, 30x augmentation\n",
    "# lmd_tokens_path = '/juice4/scr4/nlp/music/npbecker/lmd_full_tokens/07092024/06232024.autoregress.train.txt' # instr autoregressive baseline, 1x augmentation\n",
    "# lmd_tokens_path = '/nlp/scr/npbecker/lmd_valid/tokenized-events-0.txt'\n",
    "lmd_tokens_path = '/nlp/scr/npbecker/lmd_full_tokens/06042024/06042024.autoregress.train.txt' # instr baseline, 30x augmentation\n",
    "\n",
    "chunks = []\n",
    "with open(lmd_tokens_path, 'r') as file:\n",
    "    for i in range(100000):\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        chunks.append(line);\n",
    "\n",
    "tokenss = []\n",
    "start_tokens = []\n",
    "for chunk in chunks:\n",
    "    tokens = [int(tok) for tok in chunk.strip('\\n').split(' ')]\n",
    "    tokenss.append(tokens)\n",
    "    if tokens[1] == vocab['separator']:\n",
    "        start_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62d95d-299b-4d6f-b028-c99184296035",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = start_tokens[11]\n",
    "ops.print_training_tokens(t[:t.index(55026)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa6294-82ce-4d19-a1ce-8070b85a77d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = start_tokens[11]\n",
    "# visualize(ops.remove_prefix(t), 'tmp.png', vocab)\n",
    "# ops.print_training_tokens(t[:t.index(55026)+1])\n",
    "ops.print_training_tokens(t)\n",
    "e, c = ops.split(ops.remove_prefix(t))\n",
    "chords, piano = extract_instruments([t - vocab['control_offset'] for t in c], [0], as_controls=False)\n",
    "# Audio(synthesize(fs, ops.remove_prefix(t)))\n",
    "Audio(synthesize(fs, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b11b60-7247-48c9-a446-21063caeb4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove drums\n",
    "tokens = t[t.index(55026)+1:]\n",
    "prelim_e, c = ops.split(tokens)\n",
    "# get rid of drums\n",
    "e = []\n",
    "for time, dur, note in zip(prelim_e[0::3],prelim_e[1::3],prelim_e[2::3]):\n",
    "    instr = (note-NOTE_OFFSET)//2**7\n",
    "    if instr not in [128]:\n",
    "        e.extend([time, dur, note])\n",
    "\n",
    "# hack to deal with REST token at zero from time relativizing after padding\n",
    "zero_rest = e[:3]\n",
    "e = e[3:]\n",
    "# generate new control prefix without drums\n",
    "z_start, z_cont = control_prefix([32, 56], [0], 'autoregress', vocab)\n",
    "prefix = [vocab['pad']] + z_start\n",
    "# merge everything back together\n",
    "chords, human = extract_instruments([tok-CONTROL_OFFSET for tok in c], [0], as_controls=False)\n",
    "new_t, _, _ = ops.anticipate_and_anti_anticipate(e, [tok + CONTROL_OFFSET for tok in chords], [tok + CONTROL_OFFSET for tok in human])\n",
    "new_t = prefix + zero_rest + new_t\n",
    "ops.print_training_tokens(new_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66c6b1-ff99-43c3-bd7e-345292a09d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, piano))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac7385-7c19-46ed-8433-d865723053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, chords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbf5c2-faa1-4c88-9cd5-776309e470ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, new_t[new_t.index(55026)+1:301]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31141038-e499-485c-ae38-67c15ee40248",
   "metadata": {},
   "source": [
    "#### Basic autoregressive generation. To start from scratch, prompt with vocab['pad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d2827-975c-42ef-bbc4-e9d44f194714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "use_MLC = True\n",
    "\n",
    "completed_seq_length = 1 + 1*300 # should =1(mod3), and less than 1024\n",
    "top_p = .99\n",
    "temperature = 1.0\n",
    "\n",
    "# chunk = chunks[3000]\n",
    "# tokens = [int(tok) for tok in chunk.strip('\\n').split(' ')]\n",
    "# t[:t.index(55026) + 1] # get prompt from example t\n",
    "tokens = [vocab['pad']]\n",
    "\n",
    "# tokens = copy.deepcopy(list(snap[0].numpy()))\n",
    "\n",
    "# ==================================\n",
    "\n",
    "torch.manual_seed(200)\n",
    "\n",
    "while(len(tokens) < completed_seq_length):    \n",
    "    \n",
    "    new_token = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            if not use_MLC:\n",
    "                input_tokens = torch.tensor(tokens + new_token).unsqueeze(0).to(model.device)\n",
    "                logits = model(input_tokens).logits[0,-1]\n",
    "            else:\n",
    "                # MLC with no caching\n",
    "                input_tokens = torch.tensor(tokens + new_token)\n",
    "                logits, _ = debugchat_forward(model_mlc, input_tokens, None)\n",
    "                logits = torch.tensor(logits)[0,0,:]\n",
    "\n",
    "            logits = nucleus(logits, top_p)\n",
    "\n",
    "            probs = F.softmax(logits/temperature, dim=-1)\n",
    "            token = torch.multinomial(probs, 1)\n",
    "            new_token.append(int(token))\n",
    "            \n",
    "    tokens.extend(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, tokens[tokens.index(55026)+1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63c18f-8272-485b-b10d-7a153c5bcb6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ops.print_training_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ba1ad-ffa2-4840-95f8-545bcf3be7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "to = tokens[tokens.index(55026)+1:]\n",
    "e, c = ops.split(to)\n",
    "Audio(synthesize(fs, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb0c1a-2ee3-47eb-b476-5904b5f9e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, piano = extract_instruments([tok-CONTROL_OFFSET for tok in c], [0], as_controls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ff85a-8a43-4f7c-adbf-e592c2d69c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, piano))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1a79a-c26d-4780-a53f-1aa723ab7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, e + piano))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8ce77-595f-4f33-8332-96410d975c00",
   "metadata": {},
   "source": [
    "### Unconditional generation with requested instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1a189-5666-459b-8a03-b866a8f5cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events = midi_to_events('examples/chopin_test_clipped.mid', vocab)\n",
    "# mt = ops.max_time(events, seconds=True)\n",
    "# length = 50\n",
    "# tokens = generate(model_mlc, inputs=events, start_time=mt, end_time=mt+length, human_instruments=[], instruments=[0], top_p=.98, use_MLC=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f118f0c-38b5-4080-8727-4b8bee945453",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_MLC=True\n",
    "length = 50\n",
    "acc_instruments = [2, 24, 46] #list(ops.get_instruments(events).keys())[:10]\n",
    "human_instruments = [] # this is empty for instrument med baseline\n",
    "if not use_MLC:\n",
    "    unconditional_tokens = generate(model, start_time=0, end_time=length, human_instruments=human_instruments, instruments=acc_instruments, top_p=.98, use_MLC=False)\n",
    "else:\n",
    "    unconditional_tokens = generate(model_mlc, start_time=0, end_time=length, human_instruments=human_instruments, instruments=acc_instruments, top_p=.98, use_MLC=True)\n",
    "sampled_instruments = ops.get_instruments(unconditional_tokens)\n",
    "print(f'Generated {len(unconditional_tokens)} tokens.')\n",
    "print(f'Requested instruments: {sorted(acc_instruments)}')\n",
    "print(f'Sampled instruments:')\n",
    "for key in sorted(sampled_instruments.keys()):\n",
    "    print(f'    Program {key} with {sampled_instruments[key]} notes')\n",
    "print('Accuracy:')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn in acc_instruments])} instruments out of {len(acc_instruments)} requested instruments generated')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn not in acc_instruments])} instruments generated that were not requested')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11731368-f105-47ed-b00f-af16b69e5713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, unconditional_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a14750-3efe-4e4f-91b1-e827beb77e17",
   "metadata": {},
   "source": [
    "### First LIVE example: Strawberry Fields (unfinished transfer over to Live model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789ac9d-5efa-4b30-9f49-e64d6b635b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "events = midi_to_events_new('examples/strawberry.mid', vocab)\n",
    "Audio(synthesize(fs, ops.clip(events, 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e23f7-a22c-4d0e-a9e3-ea2f641f18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, generate a chord accompaniment (\"lead sheet\") and extrat the piano\n",
    "# part as simulated human input.\n",
    "# The chord accompaniment will be given to the model as anticipated controls.\n",
    "# The human accompaniment will be given to the model as anti-anticipated controls.\n",
    "human_instruments = [0]\n",
    "human_events, chord_events = extract_human_and_chords('examples/strawberry.mid', human_program_num=0, return_non_human_events=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513a07b-de8b-4a56-8649-78a42fd08145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anticipation.visuals import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a217bd0-9f29-46ef-9ded-0a48f675202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the human part (piano)\n",
    "visualize(ops.clip([tk - CONTROL_OFFSET for tk in human_events], 0, 30), '', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a918e0-6597-436f-a2f4-adcd33c658ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, ops.clip([tk - CONTROL_OFFSET for tk in human_events], 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca55790-0393-415e-bcdf-8ddcd0e39b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the genereated chord accompaniment\n",
    "visualize(ops.clip([tk - CONTROL_OFFSET for tk in chord_events], 0, 30), '', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64762f-757d-4e80-aed9-0d407206358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render chords as instrument=101 (special program num just for lead sheets)\n",
    "Audio(synthesize(fs, ops.clip([tk - CONTROL_OFFSET for tk in chord_events], 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088b4ae-21cf-4ac9-853d-ed2917875def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human part (piano) + Chords \n",
    "Audio(synthesize(fs, ops.clip(ops.sort([tk - CONTROL_OFFSET for tk in chord_events] + [tk - CONTROL_OFFSET for tk in human_events]), 0, 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179337c9-758e-4380-91b2-bb12b9dd245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are some reasonable instruments to try? Let's look at the original.\n",
    "list(ops.get_instruments(events).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f087703-beb0-4009-b940-1515763465b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 30\n",
    "requested_instruments = [25, 48]\n",
    "tokens = generate(model, chord_controls=chord_events, human_controls=human_events, start_time=0, end_time=length, human_instruments=human_instruments, instruments=requested_instruments, top_p=.98)\n",
    "print('Tokens generated: ',len(tokens))\n",
    "sampled_instruments = ops.get_instruments(tokens)\n",
    "print(f'Generated {len(tokens)} tokens.')\n",
    "print(f'Requested instruments: {sorted(requested_instruments)}')\n",
    "print(f'Sampled instruments:')\n",
    "for key in sorted(sampled_instruments.keys()):\n",
    "    print(f'    Program {key} with {sampled_instruments[key]} notes')\n",
    "print('Accuracy:')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn in requested_instruments])} instruments out of {len(requested_instruments)} requested instruments generated')\n",
    "print(f'    {len([pn for pn in sampled_instruments if pn not in requested_instruments])} instruments generated that were not requested')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b17467-6af1-4616-98fa-3660394d56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accompaniment\n",
    "Audio(synthesize(fs, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48674436-3acc-4fb2-a537-ebfbf739e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human part + Accompaniment\n",
    "Audio(synthesize(fs, ops.clip(ops.sort(tokens + [tk - CONTROL_OFFSET for tk in human_events]), 0, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4876b7-abce-47e8-bbeb-4cacb99cffbe",
   "metadata": {},
   "source": [
    "### Second Live example: jazz from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3573a-ceac-4ab8-8f70-356c136847d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"b0ea637882ee7911da70d75f0b726992.mid\"\n",
    "human_instr = 0\n",
    "original = os.path.join(\"/Users/npb/Desktop/anticipation/lmd_full/b\", filename)\n",
    "original_events = midi_to_events_new(original)\n",
    "# let's take out the drums\n",
    "original_events, _ = extract_instruments(original_events, [128])\n",
    "# remove silence in the beginning\n",
    "original_events = ops.translate(original_events, -ops.min_time(original_events, seconds=False), seconds=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb803a3-7370-45ac-8dfe-e054a29adbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, original_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99303412-986a-42e1-9d9f-0b39c75096c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_events, chord_events, agent_events = extract_human_and_chords(original, human_program_num=human_instr, return_non_human_events=True, relativize_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tvm\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from anticipation import ops\n",
    "from anticipation.config import *\n",
    "from anticipation.vocabs.tripletmidi import vocab\n",
    "\n",
    "from anticipation.sample import construct_prompt, add_token\n",
    "\n",
    "def _generate_live_chunk(\n",
    "        model, \n",
    "        start_time, \n",
    "        end_time, \n",
    "        inputs=None, \n",
    "        chord_controls=None, \n",
    "        human_controls=None, \n",
    "        instruments=None, \n",
    "        human_instruments=None, \n",
    "        top_p=1.0, \n",
    "        temperature=1.0, \n",
    "        masked_instrs=[], \n",
    "        debug=False, \n",
    "        chord_delta=DELTA*TIME_RESOLUTION, \n",
    "        human_delta=HUMAN_DELTA*TIME_RESOLUTION, \n",
    "        use_MLC=True,\n",
    "        force_z_cont=False\n",
    "    ):\n",
    "\n",
    "    if inputs is None:\n",
    "        inputs = []\n",
    "\n",
    "    if chord_controls is None:\n",
    "        chord_controls = []\n",
    "\n",
    "    if human_controls is None:\n",
    "        human_controls = []\n",
    "\n",
    "    if instruments is None:\n",
    "        raise ValueError('Must provide list of instruments')\n",
    "\n",
    "    if human_instruments is None:\n",
    "        raise ValueError('Must provide list of human instruments s')\n",
    "\n",
    "    start_time = int(TIME_RESOLUTION*start_time)\n",
    "    end_time = int(TIME_RESOLUTION*end_time)\n",
    "\n",
    "    chord_delta = DELTA*TIME_RESOLUTION\n",
    "    human_delta = HUMAN_DELTA*TIME_RESOLUTION\n",
    "\n",
    "    # prompt is events up to start_time\n",
    "    prompt = ops.pad(ops.clip(inputs, 0, start_time, seconds=False, clip_duration=False), start_time)\n",
    "\n",
    "    task = [AUTOREGRESS] # task is hardcoded to autoregress in live models\n",
    "\n",
    "    # interleave the chord_controls and human_controls with the events\n",
    "    # note that we merge future with chord_controls, as they are both anticipated\n",
    "    # tokens, controls = ops.anticipate(prompt, ops.sort(controls + [CONTROL_OFFSET+token for token in future]))\n",
    "    tokens, chord_controls, human_controls = ops.anticipate_and_anti_anticipate(prompt, chord_controls, human_controls, chord_delta=chord_delta, human_delta=human_delta)\n",
    "\n",
    "    # snap.append(construct_prompt(instruments, human_instruments, task, tokens, None, vocab, force_z_cont=force_z_cont)[0])\n",
    "\n",
    "    current_time = ops.max_time(prompt, seconds=False)\n",
    "\n",
    "    if len(tokens) > 1024:\n",
    "        print(f\"t = {current_time}, Outer loop: CONTEXT LENGTH REACHED\")\n",
    "\n",
    "    # Main generation loop\n",
    "    with tqdm(range(end_time-start_time)) as progress:\n",
    "        if chord_controls:\n",
    "            atime, adur, anote = chord_controls[0:3]\n",
    "            anticipated_tokens = chord_controls[3:]\n",
    "            anticipated_time = atime - ATIME_OFFSET\n",
    "        else:\n",
    "            # nothing to anticipate\n",
    "            anticipated_time = math.inf\n",
    "\n",
    "        if human_controls:\n",
    "            aatime, aadur, aanote = human_controls[0:3]\n",
    "            anti_anticipated_tokens = human_controls[3:]\n",
    "            anti_anticipated_time = aatime - ATIME_OFFSET\n",
    "        else:\n",
    "            # nothing to anti-anticipate\n",
    "            anti_anticipated_time = math.inf\n",
    "\n",
    "        cache = None\n",
    "        while True:\n",
    "            while (current_time >= anticipated_time - chord_delta) or (current_time >= anti_anticipated_time - human_delta):\n",
    "                if (anticipated_time - chord_delta <= anti_anticipated_time - human_delta):\n",
    "\n",
    "                    # update the cache\n",
    "                    input_ids, cache, offset = construct_prompt(instruments, human_instruments, task, tokens, cache, vocab, force_z_cont=force_z_cont)\n",
    "                    for new_token in [atime-offset, adur, anote]:\n",
    "                        with torch.no_grad():\n",
    "                            # run the model as if we were going to use its prediction\n",
    "                            if not use_MLC:\n",
    "                                input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "                                cache = model(input_ids, past_key_values=cache, use_cache=True).past_key_values\n",
    "                            else:\n",
    "                                _, cache = debugchat_forward(model, input_ids, cache)\n",
    "\n",
    "                        tokens.append(new_token)\n",
    "                        input_ids = torch.tensor([new_token])\n",
    "\n",
    "                    if len(anticipated_tokens) > 0:\n",
    "                        atime, adur, anote = anticipated_tokens[0:3]\n",
    "                        anticipated_tokens = anticipated_tokens[3:]\n",
    "                        anticipated_time = atime - ATIME_OFFSET\n",
    "                    else:\n",
    "                        # nothing more to anticipate\n",
    "                        anticipated_time = math.inf\n",
    "                else:\n",
    "                    # update the cache\n",
    "                    input_ids, cache, offset = construct_prompt(instruments, human_instruments, task, tokens, cache, vocab, force_z_cont=force_z_cont)\n",
    "                    for new_token in [aatime-offset, aadur, aanote]:\n",
    "                        with torch.no_grad():\n",
    "                            # run the model as if we were going to use its prediction\n",
    "                            if not use_MLC:\n",
    "                                input_ids = input_ids.unsqueeze(0).to(model.device)\n",
    "                                cache = model(input_ids, past_key_values=cache, use_cache=True).past_key_values\n",
    "                            else:\n",
    "                                _, cache = debugchat_forward(model, input_ids, cache)\n",
    "                        tokens.append(new_token)\n",
    "                        input_ids = torch.tensor([new_token])\n",
    "\n",
    "                    if len(anti_anticipated_tokens) > 0:\n",
    "                        aatime, aadur, aanote = anti_anticipated_tokens[0:3]\n",
    "                        anti_anticipated_tokens = anti_anticipated_tokens[3:]\n",
    "                        anti_anticipated_time = aatime - ATIME_OFFSET\n",
    "                    else:\n",
    "                        # nothing more to anti-anticipate\n",
    "                        anti_anticipated_time = math.inf\n",
    "\n",
    "            new_token, cache = add_token(model, task, tokens, instruments, human_instruments, top_p, temperature, max(start_time,current_time), masked_instrs, cache, allowed_control_pn=None, debug=False, use_MLC=use_MLC, force_z_cont=force_z_cont)\n",
    "            new_time = new_token[0] - TIME_OFFSET\n",
    "            if new_time >= end_time:\n",
    "                break\n",
    "\n",
    "            tokens.extend(new_token)\n",
    "            dt = new_time - current_time\n",
    "            assert dt >= 0\n",
    "            current_time = new_time\n",
    "            progress.update(dt)\n",
    "        \n",
    "            if len(tokens) > 1024:\n",
    "                print(f\"t = {current_time}, Inner loop: CONTEXT LENGTH REACHED\")\n",
    "\n",
    "    new_events, controls = ops.split(tokens)\n",
    "\n",
    "    new_events = ops.sort(ops.unpad(new_events))\n",
    "\n",
    "    return new_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5a65c-a1ab-45a8-9352-ed4d91f7f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(200)\n",
    "\n",
    "import time\n",
    "\n",
    "#Simulate live human input\n",
    "clock_start = time.time()\n",
    "\n",
    "simulation_start_time = 8 # NOTE: in the plugin, this function is triggered a second before simulation_start_time!\n",
    "simulation_end_time = 40\n",
    "\n",
    "GENERATION_INTERVAL = 1\n",
    "\n",
    "inputs = ops.clip(agent_events, 0, simulation_start_time, clip_duration=True, seconds=True)\n",
    "\n",
    "for st in range(simulation_start_time, simulation_end_time+1):\n",
    "\n",
    "    accompaniment = [] # TODO: the way recursive inputs are handled may need to change for generation that spans multiple context windows\n",
    "\n",
    "    start_time = st\n",
    "    end_time = st + GENERATION_INTERVAL\n",
    "\n",
    "    # Get all inputs that the plugin would see at start_time\n",
    "    human_controls = ops.clip(human_events, 0, start_time-1, clip_duration=True, seconds=True)\n",
    "\n",
    "    # Dumb heuristic to deal with streaming not being handled correctly\n",
    "    if len(inputs) + len(human_controls) + len(ops.clip(chord_controls, 0, start_time, seconds=True, clip_duration=False)) > 768:\n",
    "        force_z_cont = True\n",
    "        chord_controls = ops.clip(chord_controls, DELTA, ops.max_time(chord_controls, seconds=True), seconds=True, clip_duration=False)\n",
    "    else:\n",
    "        force_z_cont = False\n",
    "        chord_controls = chord_events\n",
    "\n",
    "    # Get all agent events that the plugin would see at start_time\n",
    "\n",
    "    instruments = sorted(list(ops.get_instruments(agent_events).keys()))\n",
    "    human_instruments = [human_instr]\n",
    "    masked_instrs=list(set(range(129)) - set(instruments))\n",
    "\n",
    "    accompaniment = _generate_live_chunk(\n",
    "        model_mlc, \n",
    "        inputs=inputs, \n",
    "        chord_controls=chord_controls, \n",
    "        human_controls=human_controls, \n",
    "        start_time=start_time, \n",
    "        end_time=end_time, \n",
    "        instruments=instruments, \n",
    "        human_instruments=human_instruments, \n",
    "        temperature=1.0,\n",
    "        top_p=.99, \n",
    "        masked_instrs=masked_instrs,\n",
    "        debug=False,\n",
    "        use_MLC=True, \n",
    "        force_z_cont=force_z_cont)\n",
    "        \n",
    "    # Recursive input: add accompaniment to inputs\n",
    "    inputs = accompaniment\n",
    "\n",
    "    clock_end = time.time()\n",
    "    generation_time = clock_end - clock_start\n",
    "    if generation_time > GENERATION_INTERVAL:\n",
    "        print(\"Generation time exceeded interval!\")\n",
    "    clock_start = clock_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c95980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(synthesize(fs, inputs + human_events))\n",
    "Audio(synthesize(fs, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio(synthesize(fs, ops.translate(ops.clip(inputs, simulation_start_time, simulation_end_time, seconds=True), -simulation_start_time, seconds=True)))\n",
    "Audio(synthesize(fs, ops.clip(inputs, simulation_start_time, simulation_end_time, seconds=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa76abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.print_tokens(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
