{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mlc_llm.testing.debug_chat import DebugChat\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLC LLM basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you would normally use MLC LLM with MLCEngine\n",
    "\n",
    "from mlc_llm import MLCEngine\n",
    "\n",
    "# Create engine\n",
    "model = \"HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC\"\n",
    "engine = MLCEngine(model)\n",
    "\n",
    "# Run chat completion in OpenAI API.\n",
    "for response in engine.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the meaning of life?\"}],\n",
    "    model=model,\n",
    "    stream=True,\n",
    "):\n",
    "    for choice in response.choices:\n",
    "        print(choice.delta.content, end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "\n",
    "engine.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DebugChat API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want access to logits, we need to hack DebugChat a bit.\n",
    "\n",
    "# First create a dummy debugging callback to avoid writing a bunch of files\n",
    "# that are otherwise written by default. This is a partial workaround, as _sample_token_from_logits()\n",
    "# in debug_chat.py still writes logits to the debug_dir given to generate().\n",
    "\n",
    "class DummyDebugInstrument:\n",
    "    def __init__(self, debug_out: Path):\n",
    "        self.debug_out = debug_out\n",
    "        pass\n",
    "\n",
    "    def reset(self, debug_out: Path):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, func, name, before_run, ret_val, *args):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then DebugChat works just like MLCChat\n",
    "\n",
    "dc = DebugChat(\n",
    "    model=\"/Users/npb/.cache/mlc_llm/model_weights/hf/mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC\",\n",
    "    debug_dir=Path(\"./debug-llama-2\"),\n",
    "    model_lib=\"/Users/npb/.cache/mlc_llm/model_lib/b571abb2b761fd7ab22fc51747ece6d7.dylib\",\n",
    "    debug_instrument=DummyDebugInstrument(Path(\"./debug-llama-2\")),\n",
    ")\n",
    "dc.generate(\"\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make music "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to convert anticipatory music transformer model weights into MLC format. As of 6/20, this is how you do it ([following these docs](https://llm.mlc.ai/docs/compilation/convert_weights.html#convert-weights-via-mlc)):\n",
    "\n",
    "**Note: q0f32 quantization seems to be unsupported by the kernel https://github.com/mlc-ai/mlc-llm/issues/2598**\n",
    "\n",
    "**Note: See an example model produced from the procedure below here: https://huggingface.co/caenopy/music-medium-800k-mlc-q0f16/. Similarly, the GPT-2 model used for debugging is here: https://huggingface.co/mlc-ai/gpt2-q0f16-MLC**\n",
    "\n",
    "0. git lfs install\n",
    "1. Clone the AMT weights https://huggingface.co/stanford-crfm/music-medium-800k\n",
    "2. Convert weights with a command like: `mlc_llm convert_weight ./models/music-medium-800k --quantization q0f16 -o ./music-medium-800k-q0f16-06262024`\n",
    "4. Copy [this tokenizer](https://huggingface.co/mlc-ai/mlc-chat-stanford-crfm-music-medium-800k-q0f32-MLC/blob/main/tokenizer.json) into the folder created by the previous command.\n",
    "3. Instead of their config utility, you can just create a file called mlc-chat-config.json in the folder created by the previous command. Write the following (changing quantization or other vocabulary attributes where appropriate) ([ref](https://huggingface.co/mlc-ai/mlc-chat-stanford-crfm-music-medium-800k-q0f32-MLC/blob/main/mlc-chat-config.json)) to the file:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"model_type\": \"gpt2\",\n",
    "  \"quantization\": \"q0f16\",\n",
    "  \"model_config\": {\n",
    "    \"vocab_size\": 55028,\n",
    "    \"n_embd\": 1024,\n",
    "    \"n_layer\": 24,\n",
    "    \"n_head\": 16,\n",
    "    \"layer_norm_epsilon\": 1e-05,\n",
    "    \"n_inner\": 4096,\n",
    "    \"context_window_size\": 1024,\n",
    "    \"prefill_chunk_size\": 1024,\n",
    "    \"scale_attn_by_inverse_layer_idx\": true,\n",
    "    \"tensor_parallel_shards\": 1,\n",
    "    \"head_dim\": 64,\n",
    "    \"max_batch_size\": 80\n",
    "  },\n",
    "  \"vocab_size\": 55028,\n",
    "  \"context_window_size\": 1024,\n",
    "  \"sliding_window_size\": -1,\n",
    "  \"prefill_chunk_size\": 1024,\n",
    "  \"attention_sink_size\": -1,\n",
    "  \"tensor_parallel_shards\": 1,\n",
    "  \"mean_gen_len\": 128,\n",
    "  \"max_gen_len\": 512,\n",
    "  \"shift_fill_factor\": 0.3,\n",
    "  \"temperature\": 1.0,\n",
    "  \"presence_penalty\": 0.0,\n",
    "  \"frequency_penalty\": 0.0,\n",
    "  \"repetition_penalty\": 1.0,\n",
    "  \"top_p\": 1.0,\n",
    "  \"conv_template\": {\n",
    "    \"name\": \"LM\",\n",
    "    \"system_template\": \"{system_message}\",\n",
    "    \"system_message\": \"\",\n",
    "    \"system_prefix_token_ids\": [\n",
    "      1\n",
    "    ],\n",
    "    \"add_role_after_system_message\": true,\n",
    "    \"roles\": {\n",
    "      \"user\": \"\",\n",
    "      \"assistant\": \"\"\n",
    "    },\n",
    "    \"role_templates\": {\n",
    "      \"user\": \"{user_message}\",\n",
    "      \"assistant\": \"{assistant_message}\",\n",
    "      \"tool\": \"{tool_message}\"\n",
    "    },\n",
    "    \"messages\": [],\n",
    "    \"seps\": [\n",
    "      \"\"\n",
    "    ],\n",
    "    \"role_content_sep\": \"\",\n",
    "    \"role_empty_sep\": \"\",\n",
    "    \"stop_str\": [\n",
    "      \"\"\n",
    "    ],\n",
    "    \"stop_token_ids\": [\n",
    "      2\n",
    "    ],\n",
    "    \"function_string\": \"\",\n",
    "    \"use_function_calling\": false\n",
    "  },\n",
    "  \"pad_token_id\": 0,\n",
    "  \"bos_token_id\": 55025,\n",
    "  \"eos_token_id\": 55025,\n",
    "  \"tokenizer_files\": [\n",
    "    \"tokenizer.json\"\n",
    "  ],\n",
    "  \"version\": \"0.1.0\"\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, generate the model library. Model libraries are platform-specific (cuda, webgpu, metal, etc.), so verify the correct commands [here](https://llm.mlc.ai/docs/compilation/compile_models.html). At the time of writing, this is how you would do it for metal:\n",
    "\n",
    "4. `mlc_llm compile ./music-medium-800k-q0f16-06262024/mlc-chat-config.json \\\n",
    "    --device metal -o ./music-medium-800k-q0f16-06262024/music-medium-800k-q0f16-metal.so`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_amt = DebugChat(\n",
    "    model=\"./mlc_music_models/music-medium-800k-q0f16-06262024\",\n",
    "    debug_dir=Path(\"./debug-anticipation\"),\n",
    "    model_lib=\"./mlc_music_models/music-medium-800k-q0f16-06262024/music-medium-800k-q0f16-metal.so\",\n",
    "    debug_instrument=DummyDebugInstrument(Path(\"./debug-anticipation\"))\n",
    ")\n",
    "\n",
    "# Let's use GPT2 for debugging...\n",
    "\n",
    "# dc_gpt2 = DebugChat(\n",
    "#     model=\"./mlc_music_models/gpt2-q0f16-MLC\",\n",
    "#     debug_dir=Path(\"./debug-anticipation\"),\n",
    "#     model_lib=\"./mlc_music_models/gpt2-q0f16-MLC/gpt2-q0f16-metal.so\",\n",
    "#     debug_instrument=DummyDebugInstrument(Path(\"./debug-anticipation\")),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generate method for music DebugChat\n",
    "\n",
    "from typing import List\n",
    "import tvm\n",
    "import numpy as np\n",
    "\n",
    "def generate(\n",
    "    dc: DebugChat,\n",
    "    input_tokens: List[str],\n",
    "    generate_length: int,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 1.0\n",
    "):\n",
    "    \"\"\"Generates the response from the model given a user prompt. User will need to\n",
    "    specify the generation length for debugging purpose. For example, a generation\n",
    "    length of 3 will include 1 prefill step and 2 decode steps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dc : DebugChat\n",
    "        The DebugChat object that contains the model and tokenizer\n",
    "        for generating the response.\n",
    "\n",
    "    generate_length : int\n",
    "        How many tokens to generate.\n",
    "        \n",
    "    input_tokens : List[str]\n",
    "        Prompt to the model.\n",
    "\n",
    "    temperature : float\n",
    "        Softmax temperature for sampling.\n",
    "        \n",
    "    top_p : float\n",
    "        Nucleus sampling parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    out_tokens = []\n",
    "\n",
    "    input_tokens = tvm.nd.array(np.array(input_tokens).astype(\"int32\"), device=dc.device)\n",
    "    embedding, input_len = dc._embed(input_tokens)\n",
    "    logits, kv_caches = dc._prefill(embedding, input_len)\n",
    "    next_token = dc._sample_token_from_logits(logits, temperature=temperature, top_p=top_p)\n",
    "    out_tokens.append(next_token)\n",
    "\n",
    "    for i in range(generate_length - 1):   \n",
    "        logits = dc._decode(next_token, kv_caches)\n",
    "\n",
    "        next_token = dc._sample_token_from_logits(logits)\n",
    "        out_tokens.append(next_token)\n",
    "\n",
    "    return out_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, here's a stripped down forward method to return logits. \n",
    "\n",
    "from typing import List\n",
    "import tvm\n",
    "import numpy as np\n",
    "\n",
    "def debugchat_forward(\n",
    "    dc: DebugChat,\n",
    "    input_tokens: List[int],\n",
    "    kv_caches: List[tvm.nd.NDArray]\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dc : DebugChat\n",
    "        The DebugChat object that contains the model and tokenizer\n",
    "        for generating the response.\n",
    "        \n",
    "    input_tokens : List[str]\n",
    "        Either a prompt to the model if kv_caches is None, or the last token.\n",
    "\n",
    "    temperature : float\n",
    "        Softmax temperature for sampling.\n",
    "        \n",
    "    top_p : float\n",
    "        Nucleus sampling parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    assert((len(input_tokens) == 1 and kv_caches is not None) or (kv_caches is None))\n",
    "\n",
    "    if kv_caches is None:\n",
    "        input_tokens = tvm.nd.array(np.array(input_tokens).astype(\"int32\"), device=dc.device)\n",
    "        embedding, input_len = dc._embed(input_tokens)\n",
    "        logits, kv_caches = dc._prefill(embedding, input_len)\n",
    "    else:\n",
    "        last_token = input_tokens[-1]\n",
    "        logits = dc._decode(last_token, kv_caches)\n",
    "    \n",
    "    return logits.numpy(), kv_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, kv_caches = debugchat_forward(dc_amt, torch.tensor([0]), kv_caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an AUTOREGRESS (55026) or ANTICIPATE (55027) token \n",
    "output = generate(dc_amt, torch.tensor([55026]), 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midi2audio\n",
    "from IPython.display import Audio\n",
    "from anticipation.convert import events_to_midi\n",
    "\n",
    "# a MIDI synthesizer\n",
    "fs = midi2audio.FluidSynth('/usr/share/sounds/sf2/FluidR3_GM.sf2')\n",
    "\n",
    "# the MIDI synthesis script\n",
    "def synthesize(fs, tokens):\n",
    "    mid = events_to_midi(tokens)\n",
    "    mid.save('tmp.mid')\n",
    "    fs.midi_to_audio('tmp.mid', 'tmp.wav')\n",
    "    return 'tmp.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(synthesize(fs, output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
